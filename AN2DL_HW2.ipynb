{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Setup\n### Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport sys\nimport os\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nIS_COLAB = 'google.colab' in sys.modules\nif IS_COLAB:\n    # %tensorflow_version only exists in Colab.\n    %tensorflow_version 2.x\n    \nimport tensorflow as tf\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom datetime import datetime\nfrom pprint import pprint\nimport time\nimport matplotlib.pyplot as plt\n\n%matplotlib notebook","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scikit-optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"import skopt\n# !pip install scikit-optimize if  necessary\nfrom skopt import gbrt_minimize, gp_minimize\nfrom skopt.utils import use_named_args\nfrom skopt.space import Real, Categorical, Integer  ","execution_count":2,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n  warnings.warn(msg, category=DeprecationWarning)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Setup GPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set GPU memory growth\n# Allows to only as much GPU memory as needed\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","execution_count":3,"outputs":[{"output_type":"stream","text":"1 Physical GPUs, 1 Logical GPUs\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1234\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\nif IS_COLAB:\n    DATASET_PATH = '/content/drive/My Drive'\nelse:\n    DATASET_PATH = '/kaggle/input/ann-and-dl-image-segmentation'\n\nCWD = os.getcwd()\nDATASET_DIR_NAME = 'Segmentation_Dataset'\nEXPERIMENTS_PATH = os.path.join(CWD, 'segmentation_experiments')\n\nEPOCHS = 150\nLEARNING_RATE = 1e-3\nSHOW_PLOTS = False\nUSE_DATA_AUGMENTATION = False\nVALIDATION_SPLIT = 0.2\nBATCH_SIZE = 4\nIMAGE_WIDTH = 256\nIMAGE_HEIGHT = 256\nOUTPUT_CHANNELS = 1\nRGB_CHANNELS = 3\nNUMBER_OF_CLASSES = 1 # house / not-house\nNN_DEPTH = 8\nSTARTING_FILTERS = 4\nEARLY_STOP = True\nSAVE_PREDICTIONS = True\nSHOW_PREDICTED_MASKS = False\nFIND_HYPERPARAMETERS = True\nDYNAMIC_SHAPE=False\nHP_CALLS = 11\n\nprint(\"cwd: %s\" % CWD)","execution_count":4,"outputs":[{"output_type":"stream","text":"cwd: /kaggle/working\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"##### Optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"dim_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform', name='learning_rate')\ndim_epochs = Integer(low=100, high=200, name=\"epochs\")\ndim_batch_size = Integer(low=2, high=64, name=\"batch_size\")\ndim_nn_depth = Integer(low=1, high=8, name=\"nn_depth\")\ndim_starting_filters = Integer(low=1, high=4, name=\"starting_filters\")\ndim_activation_fn = Categorical(categories=['relu', 'sigmoid'], name='activation_fn')\ndim_loss = Categorical(categories=['binary_crossentropy', 'sparse_categorical_crossentropy', 'dice_loss'], name='loss_fn')\ndim_optimizer = Categorical(categories=['adam', 'sgd'], name='optimizer_fn')\n\ndimensions = [dim_learning_rate, dim_epochs, dim_batch_size, dim_nn_depth, dim_starting_filters, dim_activation_fn, dim_loss, dim_optimizer]\ndefault_parameters= [LEARNING_RATE, EPOCHS, BATCH_SIZE, NN_DEPTH, STARTING_FILTERS, 'relu', 'binary_crossentropy', 'adam']\n","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Generators"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"if USE_DATA_AUGMENTATION:\n    train_img_data_gen = ImageDataGenerator(rotation_range=10,\n                                            width_shift_range=10,\n                                            height_shift_range=10,\n                                            zoom_range=0.3,\n                                            horizontal_flip=True,\n                                            vertical_flip=True,\n                                            fill_mode='constant',\n                                            cval=0,\n                                            rescale=1. / 255,\n                                            validation_split=VALIDATION_SPLIT)\n\n    train_mask_data_gen = ImageDataGenerator(rotation_range=10,\n                                             width_shift_range=10,\n                                             height_shift_range=10,\n                                             zoom_range=0.3,\n                                             horizontal_flip=True,\n                                             vertical_flip=True,\n                                             fill_mode='constant',\n                                             cval=0,\n                                             rescale=1. / 255,\n                                             validation_split=VALIDATION_SPLIT)\nelse:\n    train_img_data_gen = ImageDataGenerator(rescale=1. / 255, validation_split=VALIDATION_SPLIT)\n    train_mask_data_gen = ImageDataGenerator(rescale=1. / 255, validation_split=VALIDATION_SPLIT)\n\ntest_img_data_gen = ImageDataGenerator(rescale=1./255)\n\n# Create generators to read images from dataset directory\n# -------------------------------------------------------\n\n# path to dataset\ndataset_dir = os.path.join(DATASET_PATH, DATASET_DIR_NAME)\n# path to train dataset\ntrain_dir = os.path.join(dataset_dir, 'training')\n# path to test dataset\ntest_dir = os.path.join(dataset_dir, 'test')\n\nprint('dataset %s \\ntrain dir %s \\ntest dir %s \\n' % (dataset_dir, train_dir, test_dir))","execution_count":6,"outputs":[{"output_type":"stream","text":"dataset /kaggle/input/ann-and-dl-image-segmentation/Segmentation_Dataset \ntrain dir /kaggle/input/ann-and-dl-image-segmentation/Segmentation_Dataset/training \ntest dir /kaggle/input/ann-and-dl-image-segmentation/Segmentation_Dataset/test \n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Dataset generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Training images\ntrain_img_gen = train_img_data_gen.flow_from_directory(\n    os.path.join(train_dir, 'images'),\n    subset='training',  # subset of data\n    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n    batch_size=BATCH_SIZE,\n#     color_mode='grayscale',\n    class_mode=None,\n    shuffle=True,\n    interpolation='bilinear',\n    seed=SEED)\n## Training masks\ntrain_mask_gen = train_mask_data_gen.flow_from_directory(\n    os.path.join(train_dir, 'masks'),\n    subset='training',\n    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n    batch_size=BATCH_SIZE,\n    color_mode='grayscale',\n    class_mode=None,\n    shuffle=True,\n    interpolation='bilinear',\n    seed=SEED)\n\n## Validation images\nvalid_img_gen = train_img_data_gen.flow_from_directory(\n    os.path.join(train_dir, 'images'),\n    subset='validation',\n    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n    batch_size=BATCH_SIZE,\n#     color_mode='grayscale',\n    class_mode=None,\n    shuffle=False,\n    interpolation='bilinear',\n    seed=SEED)\n\n## Validation masks\nvalid_mask_gen = train_mask_data_gen.flow_from_directory(\n    os.path.join(train_dir, 'masks'),\n    subset='validation',\n    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n    batch_size=BATCH_SIZE,\n    color_mode='grayscale',\n    class_mode=None,\n    shuffle=False,\n    interpolation='bilinear',\n    seed=SEED)\n\n## Test images\ntest_img_gen = test_img_data_gen.flow_from_directory(os.path.join(test_dir, 'images'),\n                                                     target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n                                                     batch_size=BATCH_SIZE, \n                                                     class_mode=None, # Because we have no class subfolders in this case\n                                                     shuffle=False,\n                                                     interpolation='bilinear',\n                                                     seed=SEED)\n\n## Validation generator\nvalid_gen = zip(valid_img_gen, valid_mask_gen)\n## Training generator\ntrain_gen = zip(train_img_gen, train_mask_gen)\n## Test generator ==> since we dont have masks..\ntest_gen = test_img_gen","execution_count":7,"outputs":[{"output_type":"stream","text":"Found 6118 images belonging to 1 classes.\nFound 6118 images belonging to 1 classes.\nFound 1529 images belonging to 1 classes.\nFound 1529 images belonging to 1 classes.\nFound 1234 images belonging to 1 classes.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Create dataset objects"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_target(x_, y_):\n    y_ = tf.cast(tf.expand_dims(y_[..., 0], -1), tf.float32)\n    return x_, tf.where(y_ > 0, y_ - 1, y_ + 1)\n\ntrain_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n                                               output_types=(tf.float32, tf.float32),\n                                               output_shapes=([None, IMAGE_HEIGHT, IMAGE_WIDTH, RGB_CHANNELS], [None, IMAGE_HEIGHT, IMAGE_WIDTH, OUTPUT_CHANNELS]))\ntrain_dataset = train_dataset.map(prepare_target)\n# Repeat\ntrain_dataset = train_dataset.repeat()\n\n# Validation\n# ----------\nvalid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n                                               output_types=(tf.float32, tf.float32),\n                                               output_shapes=([None, IMAGE_HEIGHT, IMAGE_WIDTH, RGB_CHANNELS], [None, IMAGE_HEIGHT, IMAGE_WIDTH, OUTPUT_CHANNELS]))\nvalid_dataset = valid_dataset.map(prepare_target)\n# Repeat\nvalid_dataset = valid_dataset.repeat()\n\n# Test\n# ----------\ntest_dataset = tf.data.Dataset.from_generator(lambda: test_gen, \n                                               output_types=(tf.float32, tf.float32),\n                                               output_shapes=([None, IMAGE_HEIGHT, IMAGE_WIDTH, RGB_CHANNELS], [None, IMAGE_HEIGHT, IMAGE_WIDTH, OUTPUT_CHANNELS]))\ntest_dataset = test_dataset.map(prepare_target)\n# Repeat\ntest_dataset = test_dataset.repeat()","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test data generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"if SHOW_PLOTS:\n    fig, ax = plt.subplots(1, 2)\n    fig.show()\n\n    # Assign a color to each class\n    colors_dict = {}\n    colors_dict[0] = [255, 255, 255]  # foreground\n    colors_dict[1] = [0, 0, 0]  # background\n    colors_dict[2] = [3, 82, 252] # contours\n\n    iterator = iter(train_dataset)\n\n    for _ in range(2):\n        augmented_img, target = next(iterator)\n        augmented_img = augmented_img[0]   # First element\n        augmented_img = augmented_img * 255  # denormalize\n\n        target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\n\n        # Assign colors (just for visualization)\n        target_img = np.zeros([target.shape[0], target.shape[1], 3])\n\n        target_img[np.where(target == 0)] = colors_dict[0]\n        target_img[np.where(target == 1)] = colors_dict[1]\n        target_img[np.where(target == 2)] = colors_dict[2]\n\n        ax[0].imshow(np.uint8(augmented_img))\n        ax[1].imshow(np.uint8(target_img))\n\n        fig.canvas.draw()\n        time.sleep(1)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convolutional Neural Network\n### Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_coeficcient(y_true, y_pred, smooth=1):\n    from keras import backend as K\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n\n# Dice loss\ndef dice_loss(y_true, y_pred):\n    return 1-dice_coeficcient(y_true, y_pred)\n\n# IoU metric function\ndef iou_metric(y_true, y_pred):\n    # from pobability to predicted class {0, 1}\n    y_pred = tf.cast(y_pred > 0.5, tf.float32) # when using sigmoid. Use argmax for softmax\n    # A and B\n    intersection = tf.reduce_sum(y_true * y_pred)\n    # A or B\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n    # IoU\n    return intersection / union\n\n# Create a model \ndef create_model(depth, start_f, num_classes, dynamic_input_shape, activation_fn='relu'):\n    model = tf.keras.Sequential()\n\n    # Encoder\n    # -------\n    for i in range(depth):\n        if i == 0:\n            if dynamic_input_shape:\n                input_shape = [None, None, RGB_CHANNELS]\n            else:\n                input_shape = [IMAGE_HEIGHT, IMAGE_WIDTH, RGB_CHANNELS]\n        else:\n            input_shape = [None]\n\n        model.add(\n            tf.keras.layers.Conv2D(filters=start_f,\n                                   kernel_size=(3, 3),\n                                   strides=(1, 1),\n                                   padding='same',\n                                   input_shape=input_shape, \n                                   activation=activation_fn))\n#         model.add(tf.keras.layers.ReLU())\n        model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n        start_f *= 2\n\n    # Decoder\n    # -------\n    for i in range(depth):\n        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n        model.add(\n            tf.keras.layers.Conv2D(filters=start_f // 2,\n                                   kernel_size=(3, 3),\n                                   strides=(1, 1),\n                                   padding='same',\n                                   activation=activation_fn))\n\n#         model.add(tf.keras.layers.ReLU())\n\n        start_f = start_f // 2\n\n    # Prediction Layer\n    # ----------------\n    model.add(\n        tf.keras.layers.Conv2D(filters=num_classes,\n                               kernel_size=(1, 1),\n                               strides=(1, 1),\n                               padding='same',\n                               activation='sigmoid'))\n\n    return model","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find Hyperparameters\n> dimensions = [dim_learning_rate, dim_epochs, dim_batch_size, dim_nn_depth, dim_starting_filters, dim_activation_fn, dim_loss]\n> default_parameters= [LEARNING_RATE, EPOCHS, BATCH_SIZE, NN_DEPTH, STARTING_FILTERS, 'relu', 'binary_crossentropy']"},{"metadata":{"trusted":true},"cell_type":"code","source":"@use_named_args(dimensions=dimensions)\ndef fitness(learning_rate, epochs, batch_size, nn_depth, starting_filters, activation_fn, loss_fn, optimizer_fn):\n    from tensorflow.python.keras import backend as K\n    \n    number_of_classes = NUMBER_OF_CLASSES\n    if loss_fn is 'sparse_categorical_crossentropy':\n        number_of_classes = 2\n    \n    model = create_model(depth=nn_depth, \n                         start_f=starting_filters, \n                         num_classes=number_of_classes, \n                         dynamic_input_shape=DYNAMIC_SHAPE, \n                         activation_fn=activation_fn)\n    \n    # optimizer\n    if optimizer_fn is 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    elif optimizer_fn is 'sgd':\n        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.5, nesterov=True)\n        \n    # Validation metrics\n    metrics = [iou_metric, 'accuracy']\n\n    loss_function = loss_fn\n    if loss_fn is 'dice_loss':\n        # function pointer to dice_loss\n        loss_function = dice_loss\n    \n    # Compile Model\n    model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)\n    \n    callbacks= []\n    if EARLY_STOP:\n        es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_iou_metric', patience=10)\n        callbacks.append(es_callback)\n\n        \n    print('current parameters: \\n learning_rate: %d,\\n epochs: %d,\\n batch_size: %d,\\n nn_depth: %d,\\n starting_filters: %d,\\n activation_fn: %s,\\n loss_fn: %s,\\n optimizer_fn: %s' \n          % (learning_rate, epochs, batch_size, nn_depth, starting_filters, activation_fn, loss_fn, optimizer_fn))\n    blackbox = model.fit(x=train_dataset, epochs=epochs, \n                          steps_per_epoch = (len(train_img_gen)//batch_size),\n                          validation_data=valid_gen, \n                          validation_steps=(len(valid_img_gen)//batch_size), callbacks=callbacks)\n    \n    #return the validation accuracy for the last epoch.\n    accuracy = blackbox.history['val_iou_metric'][-1]\n\n    # Print the classification accuracy.\n    print()\n    print(\"IoU metric: {0:.2%}\".format(accuracy))\n    print()\n\n\n    # Delete the Keras model with these hyper-parameters from memory.\n    del model\n    \n    # Clear the Keras session, otherwise it will keep adding new\n    # models to the same TensorFlow graph each time we create\n    # a model with a different set of hyper-parameters.\n    K.clear_session()\n#     tensorflow.reset_default_graph()\n    \n    # the optimizer aims for the lowest score, so we return our negative accuracy\n    return -accuracy","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if FIND_HYPERPARAMETERS:\n    gp_result = gbrt_minimize(func=fitness,\n                              dimensions=dimensions,\n                              n_calls=HP_CALLS,\n                              n_jobs=-1,\n                              x0=default_parameters)","execution_count":null,"outputs":[{"output_type":"stream","text":"current parameters: \n learning_rate: 0,\n epochs: 150,\n batch_size: 4,\n nn_depth: 8,\n starting_filters: 4,\n activation_fn: relu,\n loss_fn: binary_crossentropy,\n optimizer_fn: adam\nTrain for 382 steps, validate for 95 steps\nEpoch 1/150\n382/382 [==============================] - 20s 53ms/step - loss: 0.6361 - iou_metric: 0.6869 - accuracy: 0.6877 - val_loss: 0.8189 - val_iou_metric: 0.3772 - val_accuracy: 0.3772\nEpoch 2/150\n382/382 [==============================] - 15s 39ms/step - loss: 0.6158 - iou_metric: 0.6973 - accuracy: 0.6973 - val_loss: 0.9181 - val_iou_metric: 0.2801 - val_accuracy: 0.2801\nEpoch 3/150\n382/382 [==============================] - 15s 39ms/step - loss: 0.6151 - iou_metric: 0.6961 - accuracy: 0.6961 - val_loss: 0.9209 - val_iou_metric: 0.2538 - val_accuracy: 0.2538\nEpoch 4/150\n382/382 [==============================] - 14s 37ms/step - loss: 0.6157 - iou_metric: 0.6963 - accuracy: 0.6963 - val_loss: 1.0236 - val_iou_metric: 0.3063 - val_accuracy: 0.3077\nEpoch 5/150\n382/382 [==============================] - 11s 29ms/step - loss: 0.6164 - iou_metric: 0.6946 - accuracy: 0.6945 - val_loss: 0.8511 - val_iou_metric: 0.3818 - val_accuracy: 0.3818\nEpoch 6/150\n382/382 [==============================] - 10s 27ms/step - loss: 0.6168 - iou_metric: 0.6939 - accuracy: 0.6939 - val_loss: 0.9424 - val_iou_metric: 0.2755 - val_accuracy: 0.2755\nEpoch 7/150\n382/382 [==============================] - 11s 29ms/step - loss: 0.6170 - iou_metric: 0.6934 - accuracy: 0.6934 - val_loss: 0.9453 - val_iou_metric: 0.2610 - val_accuracy: 0.2610\nEpoch 8/150\n382/382 [==============================] - 11s 29ms/step - loss: 0.6133 - iou_metric: 0.6975 - accuracy: 0.6975 - val_loss: 0.8863 - val_iou_metric: 0.3117 - val_accuracy: 0.3132\nEpoch 9/150\n382/382 [==============================] - 11s 27ms/step - loss: 0.6154 - iou_metric: 0.6957 - accuracy: 0.6955 - val_loss: 0.8915 - val_iou_metric: 0.3665 - val_accuracy: 0.3665\nEpoch 10/150\n382/382 [==============================] - 11s 28ms/step - loss: 0.6125 - iou_metric: 0.6986 - accuracy: 0.6986 - val_loss: 0.9673 - val_iou_metric: 0.2796 - val_accuracy: 0.2796\nEpoch 11/150\n382/382 [==============================] - 10s 27ms/step - loss: 0.6159 - iou_metric: 0.6944 - accuracy: 0.6944 - val_loss: 0.8928 - val_iou_metric: 0.2600 - val_accuracy: 0.2600\nEpoch 12/150\n382/382 [==============================] - 10s 27ms/step - loss: 0.6192 - iou_metric: 0.6910 - accuracy: 0.6910 - val_loss: 0.8874 - val_iou_metric: 0.3255 - val_accuracy: 0.3271\nEpoch 13/150\n382/382 [==============================] - 10s 27ms/step - loss: 0.6151 - iou_metric: 0.6953 - accuracy: 0.6952 - val_loss: 0.9383 - val_iou_metric: 0.3545 - val_accuracy: 0.3545\n\nIoU metric: 35.45%\n\n","name":"stdout"},{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"current parameters: \n learning_rate: 0,\n epochs: 127,\n batch_size: 55,\n nn_depth: 5,\n starting_filters: 3,\n activation_fn: relu,\n loss_fn: dice_loss,\n optimizer_fn: adam\nTrain for 27 steps, validate for 6 steps\nEpoch 1/127\n27/27 [==============================] - 2s 88ms/step - loss: 0.1358 - iou_metric: 0.6864 - accuracy: 0.6864 - val_loss: 0.2149 - val_iou_metric: 0.2735 - val_accuracy: 0.2735\nEpoch 2/127\n27/27 [==============================] - 1s 28ms/step - loss: 0.1293 - iou_metric: 0.6835 - accuracy: 0.6835 - val_loss: 0.2504 - val_iou_metric: 0.2399 - val_accuracy: 0.2399\nEpoch 3/127\n27/27 [==============================] - 1s 27ms/step - loss: 0.1243 - iou_metric: 0.7000 - accuracy: 0.7000 - val_loss: 0.2310 - val_iou_metric: 0.2927 - val_accuracy: 0.2927\nEpoch 4/127\n27/27 [==============================] - 1s 27ms/step - loss: 0.1244 - iou_metric: 0.6991 - accuracy: 0.6991 - val_loss: 0.2629 - val_iou_metric: 0.2151 - val_accuracy: 0.2151\nEpoch 5/127\n27/27 [==============================] - 1s 28ms/step - loss: 0.1249 - iou_metric: 0.6977 - accuracy: 0.6977 - val_loss: 0.2186 - val_iou_metric: 0.3788 - val_accuracy: 0.3788\nEpoch 6/127\n27/27 [==============================] - 1s 27ms/step - loss: 0.1227 - iou_metric: 0.7060 - accuracy: 0.7060 - val_loss: 0.2360 - val_iou_metric: 0.3329 - val_accuracy: 0.3329\nEpoch 7/127\n27/27 [==============================] - 1s 28ms/step - loss: 0.1300 - iou_metric: 0.6775 - accuracy: 0.6775 - val_loss: 0.2135 - val_iou_metric: 0.3196 - val_accuracy: 0.3196\nEpoch 8/127\n27/27 [==============================] - 1s 28ms/step - loss: 0.1202 - iou_metric: 0.7162 - accuracy: 0.7162 - val_loss: 0.2581 - val_iou_metric: 0.2726 - val_accuracy: 0.2726\nEpoch 9/127\n27/27 [==============================] - 1s 27ms/step - loss: 0.1293 - iou_metric: 0.6839 - accuracy: 0.6839 - val_loss: 0.2521 - val_iou_metric: 0.2346 - val_accuracy: 0.2346\nEpoch 10/127\n27/27 [==============================] - 1s 26ms/step - loss: 0.1266 - iou_metric: 0.6905 - accuracy: 0.6905 - val_loss: 0.2364 - val_iou_metric: 0.2784 - val_accuracy: 0.2784\nEpoch 11/127\n27/27 [==============================] - 1s 28ms/step - loss: 0.1247 - iou_metric: 0.6989 - accuracy: 0.6989 - val_loss: 0.2253 - val_iou_metric: 0.3488 - val_accuracy: 0.3488\nEpoch 12/127\n27/27 [==============================] - 1s 28ms/step - loss: 0.1250 - iou_metric: 0.6971 - accuracy: 0.6971 - val_loss: 0.2593 - val_iou_metric: 0.2152 - val_accuracy: 0.2152\nEpoch 13/127\n27/27 [==============================] - 1s 28ms/step - loss: 0.1270 - iou_metric: 0.6896 - accuracy: 0.6896 - val_loss: 0.2552 - val_iou_metric: 0.2114 - val_accuracy: 0.2114\nEpoch 14/127\n27/27 [==============================] - 1s 28ms/step - loss: 0.1240 - iou_metric: 0.7006 - accuracy: 0.7006 - val_loss: 0.2541 - val_iou_metric: 0.2409 - val_accuracy: 0.2409\nEpoch 15/127\n27/27 [==============================] - 1s 27ms/step - loss: 0.1251 - iou_metric: 0.6978 - accuracy: 0.6978 - val_loss: 0.2448 - val_iou_metric: 0.2685 - val_accuracy: 0.2685\nEpoch 16/127\n27/27 [==============================] - 1s 27ms/step - loss: 0.1217 - iou_metric: 0.7084 - accuracy: 0.7084 - val_loss: 0.2369 - val_iou_metric: 0.3118 - val_accuracy: 0.3118\nEpoch 17/127\n27/27 [==============================] - 1s 27ms/step - loss: 0.1331 - iou_metric: 0.6672 - accuracy: 0.6672 - val_loss: 0.2409 - val_iou_metric: 0.2372 - val_accuracy: 0.2372\nEpoch 18/127\n27/27 [==============================] - 1s 27ms/step - loss: 0.1213 - iou_metric: 0.7111 - accuracy: 0.7111 - val_loss: 0.2692 - val_iou_metric: 0.2162 - val_accuracy: 0.2162\nEpoch 19/127\n27/27 [==============================] - 1s 28ms/step - loss: 0.1193 - iou_metric: 0.7163 - accuracy: 0.7163 - val_loss: 0.2514 - val_iou_metric: 0.2845 - val_accuracy: 0.2845\nEpoch 20/127\n27/27 [==============================] - 1s 28ms/step - loss: 0.1264 - iou_metric: 0.6933 - accuracy: 0.6933 - val_loss: 0.2393 - val_iou_metric: 0.2728 - val_accuracy: 0.2728\nEpoch 21/127\n27/27 [==============================] - 1s 26ms/step - loss: 0.1273 - iou_metric: 0.6881 - accuracy: 0.6881 - val_loss: 0.2381 - val_iou_metric: 0.2835 - val_accuracy: 0.2835\nEpoch 22/127\n27/27 [==============================] - 1s 27ms/step - loss: 0.1195 - iou_metric: 0.7164 - accuracy: 0.7164 - val_loss: 0.2352 - val_iou_metric: 0.3410 - val_accuracy: 0.3410\nEpoch 23/127\n27/27 [==============================] - 1s 29ms/step - loss: 0.1228 - iou_metric: 0.7046 - accuracy: 0.7046 - val_loss: 0.2253 - val_iou_metric: 0.3435 - val_accuracy: 0.3435\n\nIoU metric: 34.35%\n\ncurrent parameters: \n learning_rate: 0,\n epochs: 128,\n batch_size: 63,\n nn_depth: 2,\n starting_filters: 1,\n activation_fn: relu,\n loss_fn: binary_crossentropy,\n optimizer_fn: adam\nTrain for 24 steps, validate for 6 steps\nEpoch 1/128\n24/24 [==============================] - 2s 69ms/step - loss: 0.6692 - iou_metric: 0.6527 - accuracy: 0.6589 - val_loss: 0.8552 - val_iou_metric: 0.2696 - val_accuracy: 0.2696\nEpoch 2/128\n24/24 [==============================] - 1s 27ms/step - loss: 0.6291 - iou_metric: 0.7158 - accuracy: 0.7158 - val_loss: 0.9112 - val_iou_metric: 0.2622 - val_accuracy: 0.2622\nEpoch 3/128\n24/24 [==============================] - 1s 26ms/step - loss: 0.6420 - iou_metric: 0.6746 - accuracy: 0.6746 - val_loss: 0.8543 - val_iou_metric: 0.2987 - val_accuracy: 0.2987\nEpoch 4/128\n24/24 [==============================] - 1s 27ms/step - loss: 0.6057 - iou_metric: 0.7086 - accuracy: 0.7086 - val_loss: 0.9254 - val_iou_metric: 0.3763 - val_accuracy: 0.3763\nEpoch 5/128\n24/24 [==============================] - 1s 27ms/step - loss: 0.6039 - iou_metric: 0.7037 - accuracy: 0.7037 - val_loss: 0.9950 - val_iou_metric: 0.3396 - val_accuracy: 0.3396\nEpoch 6/128\n24/24 [==============================] - 1s 27ms/step - loss: 0.5888 - iou_metric: 0.7143 - accuracy: 0.7143 - val_loss: 1.1257 - val_iou_metric: 0.2372 - val_accuracy: 0.2372\nEpoch 7/128\n24/24 [==============================] - 1s 26ms/step - loss: 0.6056 - iou_metric: 0.7019 - accuracy: 0.7019 - val_loss: 1.0187 - val_iou_metric: 0.2834 - val_accuracy: 0.3063\nEpoch 8/128\n24/24 [==============================] - 1s 26ms/step - loss: 0.6177 - iou_metric: 0.6733 - accuracy: 0.6733 - val_loss: 0.8601 - val_iou_metric: 0.3588 - val_accuracy: 0.3588\nEpoch 9/128\n24/24 [==============================] - 1s 27ms/step - loss: 0.5835 - iou_metric: 0.7133 - accuracy: 0.7133 - val_loss: 0.8616 - val_iou_metric: 0.4339 - val_accuracy: 0.4339\nEpoch 10/128\n24/24 [==============================] - 1s 27ms/step - loss: 0.5890 - iou_metric: 0.6854 - accuracy: 0.6854 - val_loss: 0.9166 - val_iou_metric: 0.3702 - val_accuracy: 0.3702\nEpoch 11/128\n24/24 [==============================] - 1s 26ms/step - loss: 0.5853 - iou_metric: 0.7109 - accuracy: 0.7109 - val_loss: 1.0565 - val_iou_metric: 0.1991 - val_accuracy: 0.1991\nEpoch 12/128\n24/24 [==============================] - 1s 28ms/step - loss: 0.5903 - iou_metric: 0.6950 - accuracy: 0.6950 - val_loss: 0.9521 - val_iou_metric: 0.2732 - val_accuracy: 0.2732\nEpoch 13/128\n24/24 [==============================] - 1s 26ms/step - loss: 0.5832 - iou_metric: 0.6810 - accuracy: 0.6810 - val_loss: 0.8504 - val_iou_metric: 0.3707 - val_accuracy: 0.3707\nEpoch 14/128\n24/24 [==============================] - 1s 27ms/step - loss: 0.6196 - iou_metric: 0.6553 - accuracy: 0.6553 - val_loss: 0.9233 - val_iou_metric: 0.3403 - val_accuracy: 0.3403\nEpoch 15/128\n24/24 [==============================] - 1s 28ms/step - loss: 0.5873 - iou_metric: 0.6941 - accuracy: 0.6941 - val_loss: 0.9359 - val_iou_metric: 0.4717 - val_accuracy: 0.4717\nEpoch 16/128\n24/24 [==============================] - 1s 26ms/step - loss: 0.5737 - iou_metric: 0.6965 - accuracy: 0.6965 - val_loss: 0.8603 - val_iou_metric: 0.5476 - val_accuracy: 0.5476\nEpoch 17/128\n24/24 [==============================] - 1s 28ms/step - loss: 0.5746 - iou_metric: 0.6969 - accuracy: 0.6969 - val_loss: 0.8233 - val_iou_metric: 0.5432 - val_accuracy: 0.5432\nEpoch 18/128\n24/24 [==============================] - 1s 28ms/step - loss: 0.5707 - iou_metric: 0.6894 - accuracy: 0.6894 - val_loss: 1.0269 - val_iou_metric: 0.2857 - val_accuracy: 0.2857\nEpoch 19/128\n","name":"stdout"},{"output_type":"stream","text":"24/24 [==============================] - 1s 27ms/step - loss: 0.5515 - iou_metric: 0.7022 - accuracy: 0.7022 - val_loss: 1.2358 - val_iou_metric: 0.2579 - val_accuracy: 0.2579\nEpoch 20/128\n24/24 [==============================] - 1s 27ms/step - loss: 0.5942 - iou_metric: 0.6755 - accuracy: 0.6755 - val_loss: 1.2197 - val_iou_metric: 0.3154 - val_accuracy: 0.3154\nEpoch 21/128\n24/24 [==============================] - 1s 27ms/step - loss: 0.5813 - iou_metric: 0.6859 - accuracy: 0.6859 - val_loss: 1.3387 - val_iou_metric: 0.2586 - val_accuracy: 0.2586\n\nIoU metric: 25.86%\n\ncurrent parameters: \n learning_rate: 0,\n epochs: 180,\n batch_size: 7,\n nn_depth: 3,\n starting_filters: 4,\n activation_fn: relu,\n loss_fn: sparse_categorical_crossentropy,\n optimizer_fn: sgd\nTrain for 218 steps, validate for 54 steps\nEpoch 1/180\n218/218 [==============================] - 7s 31ms/step - loss: 0.6316 - iou_metric: 0.7168 - accuracy: 0.6924 - val_loss: 0.9255 - val_iou_metric: 0.2828 - val_accuracy: 0.2828\nEpoch 2/180\n218/218 [==============================] - 6s 25ms/step - loss: 0.5990 - iou_metric: 0.6976 - accuracy: 0.6994 - val_loss: 1.0459 - val_iou_metric: 0.3075 - val_accuracy: 0.2304\nEpoch 3/180\n218/218 [==============================] - 5s 25ms/step - loss: 0.5724 - iou_metric: 0.7240 - accuracy: 0.7129 - val_loss: 1.2441 - val_iou_metric: 0.3049 - val_accuracy: 0.2454\nEpoch 4/180\n218/218 [==============================] - 5s 25ms/step - loss: 0.5596 - iou_metric: 0.7310 - accuracy: 0.7160 - val_loss: 1.1681 - val_iou_metric: 0.3611 - val_accuracy: 0.2563\nEpoch 5/180\n218/218 [==============================] - 6s 26ms/step - loss: 0.5572 - iou_metric: 0.7169 - accuracy: 0.7208 - val_loss: 1.1729 - val_iou_metric: 0.3947 - val_accuracy: 0.3014\nEpoch 6/180\n218/218 [==============================] - 6s 26ms/step - loss: 0.5436 - iou_metric: 0.7295 - accuracy: 0.7314 - val_loss: 1.2149 - val_iou_metric: 0.4180 - val_accuracy: 0.3294\nEpoch 7/180\n218/218 [==============================] - 6s 26ms/step - loss: 0.5477 - iou_metric: 0.7176 - accuracy: 0.7255 - val_loss: 1.3038 - val_iou_metric: 0.3478 - val_accuracy: 0.2446\nEpoch 8/180\n218/218 [==============================] - 6s 26ms/step - loss: 0.5433 - iou_metric: 0.7213 - accuracy: 0.7317 - val_loss: 1.3341 - val_iou_metric: 0.3243 - val_accuracy: 0.2439\nEpoch 9/180\n218/218 [==============================] - 6s 26ms/step - loss: 0.5385 - iou_metric: 0.7195 - accuracy: 0.7354 - val_loss: 1.0301 - val_iou_metric: 0.2737 - val_accuracy: 0.3257\nEpoch 10/180\n218/218 [==============================] - 5s 25ms/step - loss: 0.5371 - iou_metric: 0.7281 - accuracy: 0.7318 - val_loss: 1.3218 - val_iou_metric: 0.3109 - val_accuracy: 0.2387\nEpoch 11/180\n218/218 [==============================] - 6s 27ms/step - loss: 0.5345 - iou_metric: 0.7155 - accuracy: 0.7345 - val_loss: 1.3237 - val_iou_metric: 0.3490 - val_accuracy: 0.2545\nEpoch 12/180\n218/218 [==============================] - 6s 28ms/step - loss: 0.5371 - iou_metric: 0.7161 - accuracy: 0.7273 - val_loss: 1.4148 - val_iou_metric: 0.3603 - val_accuracy: 0.2687\nEpoch 13/180\n218/218 [==============================] - 6s 26ms/step - loss: 0.5398 - iou_metric: 0.7110 - accuracy: 0.7324 - val_loss: 1.1280 - val_iou_metric: 0.4841 - val_accuracy: 0.2752\nEpoch 14/180\n218/218 [==============================] - 6s 26ms/step - loss: 0.5178 - iou_metric: 0.7324 - accuracy: 0.7501 - val_loss: 1.3713 - val_iou_metric: 0.3077 - val_accuracy: 0.2220\nEpoch 15/180\n218/218 [==============================] - 6s 26ms/step - loss: 0.5381 - iou_metric: 0.7203 - accuracy: 0.7301 - val_loss: 1.4565 - val_iou_metric: 0.3385 - val_accuracy: 0.2520\nEpoch 16/180\n169/218 [======================>.......] - ETA: 1s - loss: 0.5253 - iou_metric: 0.7243 - accuracy: 0.7418","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best parameters found:\\n\\tlearning_rate: %f,\\n\\tepochs: %d,\\n\\tbatch_size: %d,\\n\\tnn_depth: %d,\\n\\tstarting_filters: %d,\\n\\tactivation_fn: %s,\\n\\tloss_fn: %s,\\n\\toptimizer_fn: %s' \n          % (gp_result.x[0], gp_result.x[1], gp_result.x[2], gp_result.x[3], gp_result.x[4], gp_result.x[5], gp_result.x[6], gp_result.x[7]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_classes = NUMBER_OF_CLASSES\nif gp_result.x[6] is 'sparse_categorical_crossentropy':\n    number_of_classes = 2\n        \nif FIND_HYPERPARAMETERS:\n#     [dim_learning_rate, dim_epochs, dim_batch_size, dim_nn_depth, dim_starting_filters, dim_activation_fn, dim_loss]\n    model = create_model(depth=gp_result.x[3], \n                         start_f=gp_result.x[4], \n                         num_classes=number_of_classes, \n                         dynamic_input_shape=DYNAMIC_SHAPE, \n                         activation_fn=gp_result.x[5])\nelse:\n    model = create_model(depth=NN_DEPTH,\n                         start_f=STARTING_FILTERS,\n                         num_classes=number_of_classes,\n                         dynamic_input_shape=DYNAMIC_SHAPE)\n\n# Visualize created model as a table\nmodel.summary()\n\n# Visualize initialized weights\n# print(model.weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compile the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"if FIND_HYPERPARAMETERS:\n#     [dim_learning_rate, dim_epochs, dim_batch_size, dim_nn_depth, dim_starting_filters, dim_activation_fn, dim_loss]\n    loss = gp_result.x[6]\n    if gp_result.x[7] is 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=gp_result.x[0])\n    elif gp_result.x[7] is 'sgd':\n        optimizer = tf.keras.optimizers.SGD(learning_rate=gp_result.x[0], momentum=0.5, nesterov=True)\nelse:\n    # Loss\n    # Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n    # loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    loss = 'binary_crossentropy'\n    # optimizer\n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    \n# Validation metrics\nmetrics = [iou_metric, 'accuracy']\n# Compile Model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = []\n\nif not os.path.exists(EXPERIMENTS_PATH):\n    os.makedirs(EXPERIMENTS_PATH)\n\nnow = datetime.now().strftime('%b%d_%H-%M-%S')\n\nmodel_name = 'CNN'\n\nexp_dir = os.path.join(EXPERIMENTS_PATH, model_name + '_' + str(now))\nif not os.path.exists(exp_dir):\n    os.makedirs(exp_dir)\n\n# Model checkpoint\n# ----------------\n# ckpt_dir = os.path.join(exp_dir, 'ckpts')\n# if not os.path.exists(ckpt_dir):\n#     os.makedirs(ckpt_dir)\n\n# ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n#                                                    save_weights_only=False)  # False to save the model directly\n# callbacks.append(ckpt_callback)\n\n# Visualize Learning on Tensorboard\n# ---------------------------------\n# tb_dir = os.path.join(exp_dir, 'tb_logs')\n# if not os.path.exists(tb_dir):\n#     os.makedirs(tb_dir)\n    \n# # By default shows losses and metrics for both training and validation\n# tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n#                                              profile_batch=0,\n#                                              histogram_freq=0)  # if 1 shows weights histograms\n# callbacks.append(tb_callback)\n\n# Early Stopping\n# --------------\nif EARLY_STOP:\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_iou_metric', patience=10)\n    callbacks.append(es_callback)\n\nif FIND_HYPERPARAMETERS:\n    EPOCHS = gp_result.x[1]\n    BATCH_SIZE = gp_result.x[2]\n    \nmodel.fit(x=train_dataset, epochs=EPOCHS, \n                          steps_per_epoch = (len(train_img_gen)//BATCH_SIZE),\n                          validation_data=valid_gen, \n                          validation_steps=(len(valid_img_gen)//BATCH_SIZE), \n                          callbacks=callbacks)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_output = model.evaluate(x=valid_dataset, steps=len(valid_img_gen), verbose=1)\n\nprint('Loss: %s \\nIoU Metric: %s \\nAccuracy: %s' % (evaluation_output[0], evaluation_output[1], evaluation_output[2]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final result\n### Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_competition_score(score):\n    thresholds = np.arange(0.5, 1.0, 0.05)\n    competition_score = 0\n\n    for t in thresholds:\n        if score > t:\n            competition_score += 1\n\n    competition_score /= len(thresholds)\n\n    return competition_score\n\n# Compute predictions\ndef generate_predictions(model):\n    # Cycle over test images\n    test_img_dir = os.path.join(test_dir, 'images', 'img')\n\n    # s[:10] predict until 10th image\n    image_filenames = next(os.walk(test_img_dir))[2]\n\n    results = {}\n    masks = {}\n\n    for filename in image_filenames:\n        # test images are in RGB, hence no need to transform them.\n        img = Image.open(os.path.join(test_img_dir, filename))\n        img = img.resize((IMAGE_HEIGHT, IMAGE_WIDTH))  # target size\n\n        # data_normalization\n        img_array = np.array(img)  #\n        # img_array = img_array * 1. / 255  # normalization\n        img_array = np.expand_dims(img_array, axis=0) # to fix dims of input in the model\n\n        # print(\"prediction for {}...\".format(filename))\n        predictions = model.predict(img_array)\n\n        # Get predicted class as the index corresponding to the maximum value in the vector probability\n#         predicted_mask = tf.argmax(predictions, axis=-1)\n#         predicted_mask = predicted_mask[0]\n#         target = np.array(predicted_mask)\n#         target = rle_encode(target)\n        predictions = np.round(predictions)\n        predicted_mask = predictions[0]\n        target = rle_encode(predicted_mask)\n        \n        # print(target.shape)\n        results[filename[:-4]] = target\n        masks[filename[:-4]] = predicted_mask\n\n    # create_csv(results)\n    print('Num. of labeled images', results.__len__())\n    return results, masks\n\ndef show_results(results, limit=2):\n    fig, ax = plt.subplots(1, 2)\n    fig.show()\n\n    # Assign a color to each class\n    colors_dict = {}\n    colors_dict[0] = [252, 186, 3]  # foreground\n    colors_dict[1] = [0, 0, 0]  # background\n    colors_dict[2] = [3, 82, 252] # contours\n\n    for file, mask in results.items():\n        filename = file + '.tif'\n        img_path = os.path.join(test_dir, 'images/img')\n        test_img = Image.open(os.path.join(img_path,filename)).convert('RGB')  # open as RGB\n        test_img = test_img.resize((IMAGE_HEIGHT, IMAGE_WIDTH))  # target size\n        \n        target = np.array(mask)   # First element (squeezing channel dimension)\n#         print(target.shape)\n        # Assign colors (just for visualization)\n        target_img = np.zeros([target.shape[0], target.shape[1], 3])\n\n#         target_img[np.where(target == 0)] = colors_dict[0]\n#         target_img[np.where(target == 1)] = colors_dict[1]\n#         target_img[np.where(target == 2)] = colors_dict[2]\n        \n#         print(target_img)\n\n        ax[0].imshow(np.uint8(test_img))\n        ax[1].imshow(np.uint8(target_img))\n\n        fig.canvas.draw()\n        time.sleep(1)\n    \n## Create CSV file\ndef create_csv(results, results_dir='./'):\n    csv_fname = 'results_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n    csv_fname = os.path.join(results_dir, csv_fname)\n    with open(csv_fname, 'w') as f:\n        f.write('ImageId,EncodedPixels,Width,Height\\n')\n        for key, value in results.items():\n            f.write(key + ',' + str(value) + ',' + '256' + ',' + '256' + '\\n')\n        \ndef rle_encode(img):\n    # Flatten column-wise\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions, masks = generate_predictions(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SHOW_PREDICTED_MASKS:\n    show_results(masks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Save predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"if SAVE_PREDICTIONS:\n    print('saving predictions....')\n    predictions_dir = os.path.join(CWD, 'predictions')\n    if not os.path.exists(predictions_dir):\n        os.makedirs(predictions_dir)\n    create_csv(predictions, predictions_dir)\nelse:\n#     competition score based on evaluation.\n    calculate_competition_score(evaluation_output[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_img_dir = os.path.join(test_dir, 'images', 'img')\n# # test images are in RGB, hence no need to transform them.\n# img = Image.open(os.path.join(test_img_dir, '1155.tif'))\n# img = img.resize((IMAGE_HEIGHT, IMAGE_WIDTH))  # target size\n\n# # data_normalization\n# img_array = np.array(img)  #\n# # img_array = img_array * 1. / 255  # normalization\n# img_array = np.expand_dims(img_array, axis=0) # to fix dims of input in the model\n\n# # print(\"prediction for {}...\".format(filename))\n# prediction_1155 = model.predict(img_array / 255.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction_1155 = np.round(prediction_1155)\n# prediction_1155[0]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}